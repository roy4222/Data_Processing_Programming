#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
Kaggle-Titanic: éµé”å°¼è™Ÿç”Ÿå­˜é æ¸¬å®Œæ•´å¯¦ä½œ
çµåˆæ‰€æœ‰é‡Œç¨‹ç¢‘çš„ç¶œåˆåˆ†æè…³æœ¬

æœ¬è…³æœ¬åŒ…å«å®Œæ•´çš„æ©Ÿå™¨å­¸ç¿’æµç¨‹ï¼š
1. è³‡æ–™è¼‰å…¥èˆ‡æ¢ç´¢
2. è³‡æ–™åˆ†æèˆ‡è¦–è¦ºåŒ–
3. ç‰¹å¾µå·¥ç¨‹
4. æ¨¡å‹è¨“ç·´èˆ‡é æ¸¬
5. çµæœåˆ†æèˆ‡å„ªåŒ–å»ºè­°

åƒè€ƒè³‡æ–™ï¼š
- https://hackmd.io/@Go3PyC86QhypSl7kh5nA2Q/Hk4nXFYkK
- https://medium.com/@elvennote/kaggle-titanic-machine-learning-from-disaster
- CodeSignal å’Œ GitHub æœ€ä½³å¯¦è¸
"""

# =============================================================================
# å°å…¥æ‰€æœ‰å¿…è¦çš„å¥—ä»¶
# =============================================================================
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
import warnings
import json
import re
import pickle
from datetime import datetime

# æ©Ÿå™¨å­¸ç¿’ç›¸é—œå¥—ä»¶
from sklearn.ensemble import RandomForestClassifier, RandomForestRegressor
from sklearn.model_selection import cross_val_score, StratifiedKFold, train_test_split
from sklearn.metrics import accuracy_score, classification_report, confusion_matrix
from sklearn.preprocessing import StandardScaler, LabelEncoder
from sklearn.linear_model import LogisticRegression
from sklearn.neighbors import KNeighborsClassifier
from sklearn.svm import SVC
from sklearn.naive_bayes import GaussianNB
from sklearn.tree import DecisionTreeClassifier

# è¨­å®šåœ–è¡¨åƒæ•¸å’Œè­¦å‘Šéæ¿¾
plt.rcParams['axes.unicode_minus'] = False  # ç¢ºä¿è² è™Ÿæ­£å¸¸é¡¯ç¤º
warnings.filterwarnings('ignore')

print("=" * 80)
print("ğŸš¢ Kaggle-Titanic: éµé”å°¼è™Ÿç”Ÿå­˜é æ¸¬å®Œæ•´å¯¦ä½œ")
print("ğŸ“Š çµåˆæ‰€æœ‰é‡Œç¨‹ç¢‘çš„ç¶œåˆåˆ†æ")
print("=" * 80)

# =============================================================================
# é‡Œç¨‹ç¢‘ 1: è³‡æ–™è¼‰å…¥èˆ‡æ¢ç´¢
# =============================================================================
print("\n" + "=" * 60)
print("ğŸ“Š é‡Œç¨‹ç¢‘ 1: è³‡æ–™è¼‰å…¥èˆ‡æ¢ç´¢")
print("=" * 60)

# è¼‰å…¥è³‡æ–™é›†
try:
    train_df = pd.read_csv('csv/train.csv')
    test_df = pd.read_csv('csv/test.csv')
    print("âœ… è³‡æ–™é›†è¼‰å…¥æˆåŠŸ")
    print(f"   è¨“ç·´é›†: {train_df.shape[0]} è¡Œ Ã— {train_df.shape[1]} åˆ—")
    print(f"   æ¸¬è©¦é›†: {test_df.shape[0]} è¡Œ Ã— {test_df.shape[1]} åˆ—")
except FileNotFoundError:
    print("âŒ æ‰¾ä¸åˆ°è³‡æ–™æª”æ¡ˆï¼Œè«‹ç¢ºèª csv/train.csv å’Œ csv/test.csv å­˜åœ¨")
    exit(1)

# åŸºæœ¬è³‡æ–™æ¢ç´¢
print(f"\nğŸ“‹ è³‡æ–™é›†åŸºæœ¬è³‡è¨Š:")
print(f"   è¨“ç·´é›†å¤§å°: {train_df.shape}")
print(f"   æ¸¬è©¦é›†å¤§å°: {test_df.shape}")
print(f"   ç‰¹å¾µæ¬„ä½: {list(train_df.columns)}")

# ç¼ºå¤±å€¼çµ±è¨ˆ
print(f"\nğŸ“Š è¨“ç·´é›†ç¼ºå¤±å€¼çµ±è¨ˆ:")
missing_train = train_df.isnull().sum()
for col, missing in missing_train[missing_train > 0].items():
    percentage = (missing / len(train_df)) * 100
    print(f"   {col}: {missing} ({percentage:.1f}%)")

# ç”Ÿå­˜ç‡çµ±è¨ˆ
survival_rate = train_df['Survived'].mean()
survival_count = train_df['Survived'].sum()
print(f"\nğŸ¯ æ•´é«”ç”Ÿå­˜çµ±è¨ˆ:")
print(f"   å­˜æ´»äººæ•¸: {survival_count} / {len(train_df)}")
print(f"   æ•´é«”å­˜æ´»ç‡: {survival_rate:.1%}")

# =============================================================================
# é‡Œç¨‹ç¢‘ 2: è³‡æ–™åˆ†æèˆ‡è¦–è¦ºåŒ–
# =============================================================================
print("\n" + "=" * 60)
print("ğŸ“ˆ é‡Œç¨‹ç¢‘ 2: è³‡æ–™åˆ†æèˆ‡è¦–è¦ºåŒ–")
print("=" * 60)

# åˆä½µè³‡æ–™é›†é€²è¡Œçµ±ä¸€åˆ†æ
test_df['Survived'] = np.nan
full_data = pd.concat([train_df, test_df], ignore_index=True)
print(f"âœ… è³‡æ–™åˆä½µå®Œæˆ: {full_data.shape[0]} è¡Œ")

# åˆ†æå„ç‰¹å¾µèˆ‡ç”Ÿå­˜ç‡çš„é—œä¿‚
print(f"\nğŸ“Š å„ç‰¹å¾µèˆ‡ç”Ÿå­˜ç‡åˆ†æ:")

# 1. è‰™ç­‰åˆ†æ
pclass_survival = train_df.groupby('Pclass')['Survived'].agg(['count', 'mean'])
print(f"\nğŸ« è‰™ç­‰ç”Ÿå­˜ç‡:")
for pclass in pclass_survival.index:
    count = pclass_survival.loc[pclass, 'count']
    rate = pclass_survival.loc[pclass, 'mean']
    print(f"   {pclass}ç­‰è‰™: {count:3d}äºº, å­˜æ´»ç‡ {rate:.1%}")

# 2. æ€§åˆ¥åˆ†æ
sex_survival = train_df.groupby('Sex')['Survived'].agg(['count', 'mean'])
print(f"\nğŸ‘« æ€§åˆ¥ç”Ÿå­˜ç‡:")
for sex in sex_survival.index:
    count = sex_survival.loc[sex, 'count']
    rate = sex_survival.loc[sex, 'mean']
    print(f"   {sex:6s}: {count:3d}äºº, å­˜æ´»ç‡ {rate:.1%}")

# 3. å¹´é½¡åˆ†æ
train_with_age = train_df.dropna(subset=['Age'])
age_groups = pd.cut(train_with_age['Age'], bins=[0, 12, 18, 30, 50, 100], 
                   labels=['å…’ç«¥(0-12)', 'é’å°‘å¹´(13-18)', 'é’å¹´(19-30)', 'ä¸­å¹´(31-50)', 'è€å¹´(51+)'])
age_survival = train_with_age.groupby(age_groups)['Survived'].agg(['count', 'mean'])
print(f"\nğŸ‘¶ å¹´é½¡çµ„ç”Ÿå­˜ç‡:")
for age_group in age_survival.index:
    count = age_survival.loc[age_group, 'count']
    rate = age_survival.loc[age_group, 'mean']
    print(f"   {age_group}: {count:3d}äºº, å­˜æ´»ç‡ {rate:.1%}")

# 4. å‡ºç™¼æ¸¯å£åˆ†æ
embarked_survival = train_df.groupby('Embarked')['Survived'].agg(['count', 'mean'])
print(f"\nğŸš¢ å‡ºç™¼æ¸¯å£ç”Ÿå­˜ç‡:")
port_names = {'S': 'å—å®‰æ™®æ•¦', 'C': 'ç‘Ÿå ¡', 'Q': 'çš‡åé®'}
for port in embarked_survival.index:
    if pd.notna(port):
        count = embarked_survival.loc[port, 'count']
        rate = embarked_survival.loc[port, 'mean']
        print(f"   {port}({port_names.get(port, port)}): {count:3d}äºº, å­˜æ´»ç‡ {rate:.1%}")

# =============================================================================
# è¦–è¦ºåŒ–åˆ†æåœ–è¡¨
# =============================================================================
print(f"\nğŸ“Š ç”Ÿæˆè¦–è¦ºåŒ–åœ–è¡¨...")

# è¨­å®šåœ–è¡¨æ¨£å¼
plt.style.use('default')
plt.rcParams['figure.figsize'] = (15, 10)

# å‰µå»ºç¶œåˆåˆ†æåœ–è¡¨
fig, axes = plt.subplots(2, 3, figsize=(18, 12))
fig.suptitle('Titanic Survival Analysis - Visualization Report', fontsize=16, fontweight='bold')

# 1. æ•´é«”ç”Ÿå­˜ç‡åœ“é¤…åœ–
survival_counts = train_df['Survived'].value_counts()
colors = ['#ff6b6b', '#4ecdc4']
labels = ['Died', 'Survived']
axes[0, 0].pie(survival_counts.values, labels=labels, colors=colors, autopct='%1.1f%%', 
               startangle=90, explode=(0.05, 0))
axes[0, 0].set_title('Overall Survival Distribution')

# 2. æ€§åˆ¥ç”Ÿå­˜ç‡é•·æ¢åœ–
sex_survival_data = train_df.groupby(['Sex', 'Survived']).size().unstack()
sex_survival_data.plot(kind='bar', ax=axes[0, 1], color=['#ff6b6b', '#4ecdc4'])
axes[0, 1].set_title('Survival by Gender')
axes[0, 1].set_xlabel('Gender')
axes[0, 1].set_ylabel('Count')
axes[0, 1].legend(['Died', 'Survived'])
axes[0, 1].tick_params(axis='x', rotation=0)

# 3. è‰™ç­‰ç”Ÿå­˜ç‡é•·æ¢åœ–
pclass_survival_data = train_df.groupby(['Pclass', 'Survived']).size().unstack()
pclass_survival_data.plot(kind='bar', ax=axes[0, 2], color=['#ff6b6b', '#4ecdc4'])
axes[0, 2].set_title('Survival by Passenger Class')
axes[0, 2].set_xlabel('Passenger Class')
axes[0, 2].set_ylabel('Count')
axes[0, 2].legend(['Died', 'Survived'])
axes[0, 2].tick_params(axis='x', rotation=0)

# 4. å¹´é½¡åˆ†å¸ƒç›´æ–¹åœ–
train_df['Age'].hist(bins=30, ax=axes[1, 0], alpha=0.7, color='skyblue', edgecolor='black')
axes[1, 0].set_title('Age Distribution')
axes[1, 0].set_xlabel('Age')
axes[1, 0].set_ylabel('Count')

# 5. ç¥¨åƒ¹åˆ†å¸ƒç®±ç·šåœ– (æŒ‰è‰™ç­‰)
train_df.boxplot(column='Fare', by='Pclass', ax=axes[1, 1])
axes[1, 1].set_title('Fare Distribution by Class')
axes[1, 1].set_xlabel('Passenger Class')
axes[1, 1].set_ylabel('Fare')

# 6. å‡ºç™¼æ¸¯å£ç”Ÿå­˜ç‡é•·æ¢åœ–
embarked_survival_data = train_df.groupby(['Embarked', 'Survived']).size().unstack(fill_value=0)
embarked_survival_data.plot(kind='bar', ax=axes[1, 2], color=['#ff6b6b', '#4ecdc4'])
axes[1, 2].set_title('Survival by Embarkation Port')
axes[1, 2].set_xlabel('Embarkation Port')
axes[1, 2].set_ylabel('Count')
axes[1, 2].legend(['Died', 'Survived'])
axes[1, 2].tick_params(axis='x', rotation=0)

plt.tight_layout()
plt.savefig('titanic_visualization_analysis.png', dpi=300, bbox_inches='tight')
print(f"âœ… ç¶œåˆåˆ†æåœ–è¡¨å·²å„²å­˜: titanic_visualization_analysis.png")

# å‰µå»ºè©³ç´°çš„ç”Ÿå­˜ç‡åˆ†æåœ–è¡¨
fig2, axes2 = plt.subplots(2, 2, figsize=(15, 10))
fig2.suptitle('Detailed Survival Analysis', fontsize=16, fontweight='bold')

# 1. æ€§åˆ¥ x è‰™ç­‰ ç”Ÿå­˜ç‡ç†±åŠ›åœ–
pivot_table = train_df.pivot_table(values='Survived', index='Sex', columns='Pclass', aggfunc='mean')
sns.heatmap(pivot_table, annot=True, fmt='.2f', cmap='RdYlGn', ax=axes2[0, 0])
axes2[0, 0].set_title('Survival Rate by Gender x Class')

# 2. å¹´é½¡çµ„ç”Ÿå­˜ç‡
train_with_age = train_df.dropna(subset=['Age'])
age_groups = pd.cut(train_with_age['Age'], bins=[0, 12, 18, 30, 50, 100], 
                   labels=['Child\n(0-12)', 'Teen\n(13-18)', 'Young Adult\n(19-30)', 'Middle Age\n(31-50)', 'Senior\n(51+)'])
age_survival_rate = train_with_age.groupby(age_groups)['Survived'].mean()
age_survival_rate.plot(kind='bar', ax=axes2[0, 1], color='lightcoral')
axes2[0, 1].set_title('Survival Rate by Age Group')
axes2[0, 1].set_ylabel('Survival Rate')
axes2[0, 1].tick_params(axis='x', rotation=45)

# 3. å®¶åº­å¤§å° vs ç”Ÿå­˜ç‡
train_df['FamilySize'] = train_df['SibSp'] + train_df['Parch'] + 1
family_survival = train_df.groupby('FamilySize')['Survived'].agg(['count', 'mean'])
family_survival['mean'].plot(kind='bar', ax=axes2[1, 0], color='lightgreen')
axes2[1, 0].set_title('Survival Rate by Family Size')
axes2[1, 0].set_xlabel('Family Size')
axes2[1, 0].set_ylabel('Survival Rate')

# 4. ç¥¨åƒ¹å€é–“ vs ç”Ÿå­˜ç‡
fare_bins = pd.qcut(train_df['Fare'], q=5, labels=['Very Low', 'Low', 'Medium', 'High', 'Very High'])
fare_survival = train_df.groupby(fare_bins)['Survived'].mean()
fare_survival.plot(kind='bar', ax=axes2[1, 1], color='gold')
axes2[1, 1].set_title('Survival Rate by Fare Range')
axes2[1, 1].set_xlabel('Fare Range')
axes2[1, 1].set_ylabel('Survival Rate')
axes2[1, 1].tick_params(axis='x', rotation=45)

plt.tight_layout()
plt.savefig('titanic_detailed_survival_analysis.png', dpi=300, bbox_inches='tight')
print(f"âœ… è©³ç´°ç”Ÿå­˜ç‡åˆ†æåœ–è¡¨å·²å„²å­˜: titanic_detailed_survival_analysis.png")

# å‰µå»ºç›¸é—œæ€§åˆ†æåœ–è¡¨
fig3, axes3 = plt.subplots(1, 2, figsize=(15, 6))
fig3.suptitle('Feature Correlation Analysis', fontsize=16, fontweight='bold')

# æº–å‚™æ•¸å€¼åŒ–çš„è³‡æ–™é€²è¡Œç›¸é—œæ€§åˆ†æ
correlation_data = train_df.copy()
correlation_data['Sex_num'] = correlation_data['Sex'].map({'male': 0, 'female': 1})
correlation_data['Embarked_num'] = correlation_data['Embarked'].map({'S': 0, 'C': 1, 'Q': 2})

# é¸æ“‡æ•¸å€¼ç‰¹å¾µé€²è¡Œç›¸é—œæ€§åˆ†æ
numeric_features = ['Survived', 'Pclass', 'Sex_num', 'Age', 'SibSp', 'Parch', 'Fare', 'Embarked_num']
corr_matrix = correlation_data[numeric_features].corr()

# 1. ç›¸é—œæ€§ç†±åŠ›åœ–
sns.heatmap(corr_matrix, annot=True, cmap='coolwarm', center=0, ax=axes3[0])
axes3[0].set_title('Feature Correlation Matrix')

# 2. èˆ‡ç”Ÿå­˜ç‡çš„ç›¸é—œæ€§é•·æ¢åœ–
survival_corr = corr_matrix['Survived'].drop('Survived').sort_values(key=abs, ascending=False)
colors = ['red' if x < 0 else 'green' for x in survival_corr.values]
survival_corr.plot(kind='bar', ax=axes3[1], color=colors)
axes3[1].set_title('Feature Correlation with Survival')
axes3[1].set_ylabel('Correlation Coefficient')
axes3[1].axhline(y=0, color='black', linestyle='-', linewidth=0.5)
axes3[1].tick_params(axis='x', rotation=45)

plt.tight_layout()
plt.savefig('titanic_correlation_analysis.png', dpi=300, bbox_inches='tight')
print(f"âœ… ç›¸é—œæ€§åˆ†æåœ–è¡¨å·²å„²å­˜: titanic_correlation_analysis.png")

# é—œé–‰åœ–è¡¨ä»¥é¿å…é˜»å¡ç¨‹å¼åŸ·è¡Œ
plt.close('all')
print(f"\nğŸ“Š è¦–è¦ºåŒ–åˆ†æå®Œæˆï¼å·²ç”Ÿæˆ 3 å€‹åœ–è¡¨æª”æ¡ˆ:")

# =============================================================================
# é‡Œç¨‹ç¢‘ 3: ç‰¹å¾µå·¥ç¨‹
# =============================================================================
print("\n" + "=" * 60)
print("ğŸ”§ é‡Œç¨‹ç¢‘ 3: ç‰¹å¾µå·¥ç¨‹")
print("=" * 60)

# å‰µå»ºç‰¹å¾µå·¥ç¨‹å¾Œçš„è³‡æ–™å‰¯æœ¬
data_processed = full_data.copy()

# 1. ç¨±è¬‚æå–
def extract_title(name):
    """å¾å§“åä¸­æå–ç¨±è¬‚"""
    title_search = re.search(' ([A-Za-z]+)\.', name)
    if title_search:
        return title_search.group(1)
    return ""

data_processed['Title'] = data_processed['Name'].apply(extract_title)

# åˆä½µç¨€æœ‰ç¨±è¬‚
def map_title(title):
    """åˆä½µç¨€æœ‰ç¨±è¬‚"""
    title_mapping = {
        'Mr': 'Mr', 'Miss': 'Miss', 'Mrs': 'Mrs', 'Master': 'Master',
        'Dr': 'Rare', 'Rev': 'Rare', 'Col': 'Rare', 'Major': 'Rare',
        'Mlle': 'Miss', 'Countess': 'Rare', 'Ms': 'Miss', 'Lady': 'Rare',
        'Jonkheer': 'Rare', 'Don': 'Rare', 'Dona': 'Rare', 'Mme': 'Mrs',
        'Capt': 'Rare', 'Sir': 'Rare'
    }
    return title_mapping.get(title, 'Rare')

data_processed['Title_Clean'] = data_processed['Title'].apply(map_title)
print("âœ… ç¨±è¬‚æå–å®Œæˆ")

# 2. ç¥¨è™Ÿè™•ç†
def extract_ticket_prefix(ticket):
    """æå–ç¥¨è™Ÿå‰ç¶´"""
    if pd.isna(ticket):
        return 'Unknown'
    ticket = str(ticket).replace(' ', '').replace('.', '').replace('/', '').upper()
    match = re.match(r'([A-Z]+)', ticket)
    if match:
        return match.group(1)
    else:
        return 'X'

data_processed['Ticket_Prefix'] = data_processed['Ticket'].apply(extract_ticket_prefix)
print("âœ… ç¥¨è™Ÿè™•ç†å®Œæˆ")

# 3. å®¢è‰™è™•ç†
def extract_cabin_class(cabin):
    """æå–å®¢è‰™ç­‰ç´š"""
    if pd.isna(cabin):
        return 'NoCabin'
    cabin_letter = str(cabin)[0]
    return cabin_letter if cabin_letter.isalpha() else 'NoCabin'

data_processed['Cabin_Class'] = data_processed['Cabin'].apply(extract_cabin_class)
print("âœ… å®¢è‰™è™•ç†å®Œæˆ")

# 4. å®¶åº­å¤§å°ç‰¹å¾µ
data_processed['FamilySize'] = data_processed['SibSp'] + data_processed['Parch'] + 1
data_processed['IsAlone'] = (data_processed['FamilySize'] == 1).astype(int)

def categorize_family_size(size):
    """å®¶åº­å¤§å°åˆ†é¡"""
    if size == 1:
        return 'Alone'
    elif size <= 4:
        return 'Small'
    else:
        return 'Large'

data_processed['FamilySize_Group'] = data_processed['FamilySize'].apply(categorize_family_size)
print("âœ… å®¶åº­å¤§å°ç‰¹å¾µå®Œæˆ")

# 5. ç¼ºå¤±å€¼è™•ç†
print(f"\nğŸ”§ ç¼ºå¤±å€¼è™•ç†:")

# Embarked ç¼ºå¤±å€¼è™•ç†
mode_embarked = data_processed['Embarked'].mode()[0]
data_processed['Embarked'].fillna(mode_embarked, inplace=True)
print(f"   Embarked: ç”¨ '{mode_embarked}' å¡«è£œ")

# Fare ç¼ºå¤±å€¼è™•ç†
median_fare = data_processed['Fare'].median()
data_processed['Fare'].fillna(median_fare, inplace=True)
print(f"   Fare: ç”¨ä¸­ä½æ•¸ {median_fare:.2f} å¡«è£œ")

# Age ç¼ºå¤±å€¼è™•ç† - ä½¿ç”¨éš¨æ©Ÿæ£®æ—é æ¸¬
print(f"   Age: ä½¿ç”¨éš¨æ©Ÿæ£®æ—é æ¸¬...")
age_features = ['Pclass', 'SibSp', 'Parch', 'Fare', 'FamilySize']

# å‰µå»ºæ•¸å€¼ç·¨ç¢¼
data_processed['Sex_num'] = data_processed['Sex'].map({'male': 0, 'female': 1})
data_processed['Embarked_num'] = data_processed['Embarked'].map({'S': 0, 'C': 1, 'Q': 2})
data_processed['Title_num'] = data_processed['Title_Clean'].map({
    'Mr': 0, 'Miss': 1, 'Mrs': 2, 'Master': 3, 'Rare': 4
})

age_features.extend(['Sex_num', 'Embarked_num', 'Title_num'])

known_age = data_processed[data_processed['Age'].notna()]
unknown_age = data_processed[data_processed['Age'].isna()]

if len(unknown_age) > 0:
    rf_age = RandomForestRegressor(n_estimators=100, random_state=42)
    rf_age.fit(known_age[age_features], known_age['Age'])
    predicted_ages = rf_age.predict(unknown_age[age_features])
    data_processed.loc[data_processed['Age'].isna(), 'Age'] = predicted_ages
    print(f"   Age: é æ¸¬äº† {len(unknown_age)} å€‹ç¼ºå¤±å€¼")

# 6. å‰µå»ºè¡ç”Ÿç‰¹å¾µ
def categorize_age(age):
    """å¹´é½¡åˆ†çµ„"""
    if age <= 12:
        return 'Child'
    elif age <= 18:
        return 'Teen'
    elif age <= 30:
        return 'Young_Adult'
    elif age <= 50:
        return 'Middle_Age'
    else:
        return 'Senior'

data_processed['Age_Group'] = data_processed['Age'].apply(categorize_age)

# ç¥¨åƒ¹åˆ†çµ„
data_processed['Fare_Group'] = pd.qcut(data_processed['Fare'], q=4, 
                                      labels=['Low', 'Medium_Low', 'Medium_High', 'High'])

print("âœ… è¡ç”Ÿç‰¹å¾µå‰µå»ºå®Œæˆ")

# 7. é¡åˆ¥è®Šæ•¸ç·¨ç¢¼
categorical_columns = ['Sex', 'Embarked', 'Title_Clean', 'Cabin_Class', 
                      'FamilySize_Group', 'Age_Group', 'Fare_Group']

label_mappings = {}
for col in categorical_columns:
    if col in data_processed.columns:
        unique_values = data_processed[col].unique()
        mapping = {val: idx for idx, val in enumerate(unique_values)}
        label_mappings[col] = mapping
        data_processed[f'{col}_encoded'] = data_processed[col].map(mapping)

print("âœ… é¡åˆ¥è®Šæ•¸ç·¨ç¢¼å®Œæˆ")

# æº–å‚™æœ€çµ‚ç‰¹å¾µ
final_features = [
    'Pclass', 'Age', 'SibSp', 'Parch', 'Fare', 'FamilySize', 'IsAlone',
    'Sex_encoded', 'Embarked_encoded', 'Title_Clean_encoded',
    'Cabin_Class_encoded', 'FamilySize_Group_encoded',
    'Age_Group_encoded', 'Fare_Group_encoded'
]

# åˆ†é›¢è¨“ç·´å’Œæ¸¬è©¦è³‡æ–™
train_processed = data_processed[:len(train_df)].copy()
test_processed = data_processed[len(train_df):].copy()

print(f"\nğŸ“Š ç‰¹å¾µå·¥ç¨‹å®Œæˆ:")
print(f"   æœ€çµ‚ç‰¹å¾µæ•¸é‡: {len(final_features)}")
print(f"   è¨“ç·´é›†: {train_processed.shape[0]} è¡Œ")
print(f"   æ¸¬è©¦é›†: {test_processed.shape[0]} è¡Œ")

# =============================================================================
# é‡Œç¨‹ç¢‘ 4: æ¨¡å‹è¨“ç·´èˆ‡é æ¸¬
# =============================================================================
print("\n" + "=" * 60)
print("ğŸ¤– é‡Œç¨‹ç¢‘ 4: æ¨¡å‹è¨“ç·´èˆ‡é æ¸¬")
print("=" * 60)

# æº–å‚™è¨“ç·´è³‡æ–™
X_train = train_processed[final_features]
y_train = train_processed['Survived']
X_test = test_processed[final_features]

print(f"âœ… è¨“ç·´è³‡æ–™æº–å‚™å®Œæˆ:")
print(f"   X_train: {X_train.shape}")
print(f"   y_train: {y_train.shape}")
print(f"   X_test: {X_test.shape}")

# å¤šæ¨¡å‹æ¯”è¼ƒå‡½æ•¸
def train_multiple_models(X_train, y_train):
    """è¨“ç·´å¤šå€‹æ©Ÿå™¨å­¸ç¿’æ¨¡å‹"""
    models = {}
    
    # 1. é‚è¼¯å›æ­¸
    models['Logistic Regression'] = LogisticRegression(random_state=42)
    
    # 2. Kè¿‘é„°
    models['K-Neighbors'] = KNeighborsClassifier(n_neighbors=5)
    
    # 3. æ”¯æŒå‘é‡æ©Ÿ (ç·šæ€§)
    models['SVM Linear'] = SVC(kernel='linear', random_state=42)
    
    # 4. æ”¯æŒå‘é‡æ©Ÿ (RBF)
    models['SVM RBF'] = SVC(kernel='rbf', random_state=42)
    
    # 5. é«˜æ–¯æ¨¸ç´ è²è‘‰æ–¯
    models['Gaussian NB'] = GaussianNB()
    
    # 6. æ±ºç­–æ¨¹
    models['Decision Tree'] = DecisionTreeClassifier(criterion='entropy', random_state=42)
    
    # 7. éš¨æ©Ÿæ£®æ—
    models['Random Forest'] = RandomForestClassifier(n_estimators=100, random_state=42)
    
    # è¨“ç·´æ‰€æœ‰æ¨¡å‹
    trained_models = {}
    print(f"\nğŸš€ é–‹å§‹è¨“ç·´å¤šå€‹æ¨¡å‹...")
    
    for name, model in models.items():
        model.fit(X_train, y_train)
        train_score = model.score(X_train, y_train)
        trained_models[name] = model
        print(f"   {name}: è¨“ç·´æº–ç¢ºç‡ {train_score:.4f} ({train_score:.1%})")
    
    return trained_models

# è¨“ç·´å¤šå€‹æ¨¡å‹
all_models = train_multiple_models(X_train, y_train)

# äº¤å‰é©—è­‰è©•ä¼°
print(f"\nğŸ”„ 5æŠ˜äº¤å‰é©—è­‰è©•ä¼°:")
cv_results = {}
skf = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)

for name, model in all_models.items():
    cv_scores = cross_val_score(model, X_train, y_train, cv=skf, scoring='accuracy')
    cv_results[name] = {
        'mean': cv_scores.mean(),
        'std': cv_scores.std(),
        'scores': cv_scores
    }
    print(f"   {name:18s}: {cv_scores.mean():.4f} Â± {cv_scores.std():.4f}")

# é¸æ“‡æœ€ä½³æ¨¡å‹
best_model_name = max(cv_results.keys(), key=lambda x: cv_results[x]['mean'])
best_model = all_models[best_model_name]
best_cv_score = cv_results[best_model_name]['mean']

print(f"\nğŸ† æœ€ä½³æ¨¡å‹: {best_model_name}")
print(f"   äº¤å‰é©—è­‰æº–ç¢ºç‡: {best_cv_score:.4f} ({best_cv_score:.1%})")

# ç‰¹å¾µé‡è¦æ€§åˆ†æ (é‡å° Random Forest)
if 'Random Forest' in all_models:
    rf_model = all_models['Random Forest']
    feature_importance = pd.DataFrame({
        'feature': final_features,
        'importance': rf_model.feature_importances_
    }).sort_values('importance', ascending=False)
    
    print(f"\nğŸ” éš¨æ©Ÿæ£®æ—ç‰¹å¾µé‡è¦æ€§ (å‰5å):")
    for i, (_, row) in enumerate(feature_importance.head(5).iterrows(), 1):
        print(f"   {i}. {row['feature']:20s}: {row['importance']:.3f} ({row['importance']*100:.1f}%)")

# ä½¿ç”¨æœ€ä½³æ¨¡å‹é€²è¡Œé æ¸¬
print(f"\nğŸ¯ ä½¿ç”¨æœ€ä½³æ¨¡å‹é€²è¡Œæ¸¬è©¦é›†é æ¸¬...")
test_predictions = best_model.predict(X_test)
test_probabilities = best_model.predict_proba(X_test)[:, 1] if hasattr(best_model, 'predict_proba') else None

survival_count = test_predictions.sum()
survival_rate = test_predictions.mean()

print(f"âœ… é æ¸¬å®Œæˆ:")
print(f"   é æ¸¬å­˜æ´»äººæ•¸: {int(survival_count)} / {len(test_predictions)}")
print(f"   é æ¸¬å­˜æ´»ç‡: {survival_rate:.1%}")

# =============================================================================
# é‡Œç¨‹ç¢‘ 5: çµæœåˆ†æèˆ‡æ–‡ä»¶ç”Ÿæˆ
# =============================================================================
print("\n" + "=" * 60)
print("ğŸ“‹ é‡Œç¨‹ç¢‘ 5: çµæœåˆ†æèˆ‡æ–‡ä»¶ç”Ÿæˆ")
print("=" * 60)

# ç”Ÿæˆæäº¤æª”æ¡ˆ
original_test = pd.read_csv('csv/test.csv')
submission = pd.DataFrame({
    'PassengerId': original_test['PassengerId'],
    'Survived': test_predictions.astype(int)
})

submission_filename = 'titanic_complete_submission.csv'
submission.to_csv(submission_filename, index=False)
print(f"âœ… æäº¤æª”æ¡ˆå·²ç”Ÿæˆ: {submission_filename}")

# å„²å­˜æœ€ä½³æ¨¡å‹
model_filename = f'titanic_best_model_{best_model_name.replace(" ", "_").lower()}.pkl'
with open(model_filename, 'wb') as f:
    pickle.dump(best_model, f)
print(f"âœ… æœ€ä½³æ¨¡å‹å·²å„²å­˜: {model_filename}")

# ç”Ÿæˆè©³ç´°åˆ†æå ±å‘Š
analysis_report = {
    "å°ˆæ¡ˆåç¨±": "Kaggle-Titanic: éµé”å°¼è™Ÿç”Ÿå­˜é æ¸¬å®Œæ•´å¯¦ä½œ",
    "å®Œæˆæ™‚é–“": datetime.now().strftime("%Y-%m-%d %H:%M:%S"),
    "è³‡æ–™çµ±è¨ˆ": {
        "è¨“ç·´é›†å¤§å°": len(train_df),
        "æ¸¬è©¦é›†å¤§å°": len(test_df),
        "åŸå§‹ç‰¹å¾µæ•¸": len(train_df.columns) - 1,
        "æœ€çµ‚ç‰¹å¾µæ•¸": len(final_features),
        "æ•´é«”å­˜æ´»ç‡": f"{survival_rate:.1%}"
    },
    "æ¨¡å‹æ¯”è¼ƒçµæœ": {name: f"{result['mean']:.4f} Â± {result['std']:.4f}" 
                    for name, result in cv_results.items()},
    "æœ€ä½³æ¨¡å‹": {
        "æ¨¡å‹åç¨±": best_model_name,
        "äº¤å‰é©—è­‰åˆ†æ•¸": f"{best_cv_score:.4f}",
        "é æ¸¬å­˜æ´»ç‡": f"{survival_rate:.1%}"
    },
    "ç‰¹å¾µé‡è¦æ€§": feature_importance.head(10).to_dict('records') if 'Random Forest' in all_models else [],
    "é—œéµç™¼ç¾": {
        "æ€§åˆ¥å½±éŸ¿": f"å¥³æ€§å­˜æ´»ç‡ {sex_survival.loc['female', 'mean']:.1%}, ç”·æ€§ {sex_survival.loc['male', 'mean']:.1%}",
        "è‰™ç­‰å½±éŸ¿": f"1ç­‰è‰™ {pclass_survival.loc[1, 'mean']:.1%}, 3ç­‰è‰™ {pclass_survival.loc[3, 'mean']:.1%}",
        "å¹´é½¡å½±éŸ¿": "å…’ç«¥å­˜æ´»ç‡è¼ƒé«˜",
        "ç‰¹å¾µå·¥ç¨‹": f"å¾ {len(train_df.columns)-1} å€‹åŸå§‹ç‰¹å¾µæ“´å±•åˆ° {len(final_features)} å€‹"
    }
}

report_filename = 'titanic_complete_analysis_report.json'
with open(report_filename, 'w', encoding='utf-8') as f:
    json.dump(analysis_report, f, ensure_ascii=False, indent=2)
print(f"âœ… åˆ†æå ±å‘Šå·²ç”Ÿæˆ: {report_filename}")

# æ€§èƒ½æ”¹é€²å»ºè­°
print(f"\nğŸ’¡ æ¨¡å‹å„ªåŒ–å»ºè­°:")
train_score = best_model.score(X_train, y_train)
overfitting = train_score - best_cv_score

if overfitting > 0.1:
    print("   âš ï¸  åš´é‡éæ“¬åˆ:")
    print("      - å¢åŠ æ­£å‰‡åŒ–åƒæ•¸")
    print("      - æ¸›å°‘æ¨¡å‹è¤‡é›œåº¦")
    print("      - ä½¿ç”¨æ›´å¤šæ•¸æ“š")
elif overfitting > 0.05:
    print("   âš¡ ä¸­åº¦éæ“¬åˆ:")
    print("      - èª¿æ•´è¶…åƒæ•¸")
    print("      - ç‰¹å¾µé¸æ“‡")
else:
    print("   âœ… æ¨¡å‹æ³›åŒ–è‰¯å¥½")

print(f"\nğŸš€ é€²éšå„ªåŒ–æ–¹å‘:")
print("   1. è¶…åƒæ•¸èª¿å„ª (GridSearchCV)")
print("   2. ç‰¹å¾µé¸æ“‡å’Œé™ç¶­")
print("   3. æ¨¡å‹èåˆ (Ensemble)")
print("   4. æ·±åº¦å­¸ç¿’æ–¹æ³•")

# æœ€çµ‚ç¸½çµ
print(f"\n" + "=" * 80)
print("ğŸ‰ éµé”å°¼è™Ÿç”Ÿå­˜é æ¸¬å®Œæ•´åˆ†æå®Œæˆï¼")
print("=" * 80)

print(f"\nğŸ“Š æœ€çµ‚æˆæœç¸½è¦½:")
print(f"   ğŸ”¢ è™•ç†è³‡æ–™: {len(full_data):,} ç­†è¨˜éŒ„")
print(f"   ğŸ§ª ç‰¹å¾µå·¥ç¨‹: {len(train_df.columns)-1} â†’ {len(final_features)} å€‹ç‰¹å¾µ")
print(f"   ğŸ¤– æœ€ä½³æ¨¡å‹: {best_model_name}")
print(f"   ğŸ¯ é æ¸¬æº–ç¢ºç‡: {best_cv_score:.1%} (äº¤å‰é©—è­‰)")
print(f"   ğŸ“ˆ é æ¸¬å­˜æ´»ç‡: {survival_rate:.1%}")

print(f"\nğŸ“ ç”Ÿæˆæª”æ¡ˆ:")
print(f"   â€¢ {submission_filename} - Kaggle æäº¤æª”æ¡ˆ")
print(f"   â€¢ {model_filename} - æœ€ä½³è¨“ç·´æ¨¡å‹")
print(f"   â€¢ {report_filename} - è©³ç´°åˆ†æå ±å‘Š")

print(f"\nğŸ¯ ä¸‹ä¸€æ­¥è¡Œå‹•:")
print("   1. ğŸ“¤ æäº¤çµæœåˆ° Kaggle ç«¶è³½")
print("   2. ğŸ“Š åˆ†ææ’è¡Œæ¦œè¡¨ç¾")
print("   3. ğŸ”§ æ ¹æ“šåé¥‹å„ªåŒ–æ¨¡å‹")
print("   4. ğŸ’¡ å˜—è©¦æ›´é€²éšçš„æŠ€è¡“")

print(f"\nğŸ† æ­å–œå®Œæˆå®Œæ•´çš„æ©Ÿå™¨å­¸ç¿’å°ˆæ¡ˆï¼")
print("   é€™å€‹è…³æœ¬å±•ç¤ºäº†å¾è³‡æ–™æ¢ç´¢åˆ°æ¨¡å‹éƒ¨ç½²çš„å®Œæ•´æµç¨‹ã€‚")
print("   æ‚¨å¯ä»¥åŸºæ–¼é€™å€‹åŸºç¤ç¹¼çºŒå„ªåŒ–å’Œæ”¹é€²æ¨¡å‹æ€§èƒ½ã€‚")
print("\n" + "=" * 80)

# å¯é¸ï¼šé¡¯ç¤ºæœ€ä½³æ¨¡å‹çš„è©³ç´°è©•ä¼°
if input("\næ˜¯å¦é¡¯ç¤ºæœ€ä½³æ¨¡å‹çš„è©³ç´°è©•ä¼°ï¼Ÿ(y/n): ").lower() == 'y':
    # åˆ†å‰²ä¸€éƒ¨åˆ†æ•¸æ“šé€²è¡Œè©³ç´°è©•ä¼°
    X_train_split, X_val_split, y_train_split, y_val_split = train_test_split(
        X_train, y_train, test_size=0.2, random_state=42, stratify=y_train
    )
    
    # é‡æ–°è¨“ç·´æœ€ä½³æ¨¡å‹
    best_model.fit(X_train_split, y_train_split)
    y_val_pred = best_model.predict(X_val_split)
    
    print(f"\nğŸ“Š æœ€ä½³æ¨¡å‹è©³ç´°è©•ä¼°:")
    print(f"   é©—è­‰é›†æº–ç¢ºç‡: {accuracy_score(y_val_split, y_val_pred):.4f}")
    
    # æ··æ·†çŸ©é™£
    cm = confusion_matrix(y_val_split, y_val_pred)
    print(f"\nğŸ“‹ æ··æ·†çŸ©é™£:")
    print(f"           é æ¸¬")
    print(f"å¯¦éš›    æ­»äº¡  å­˜æ´»")
    print(f"æ­»äº¡    {cm[0,0]:3d}   {cm[0,1]:3d}")
    print(f"å­˜æ´»    {cm[1,0]:3d}   {cm[1,1]:3d}")
    
    # åˆ†é¡å ±å‘Š
    print(f"\nğŸ“ˆ è©³ç´°åˆ†é¡å ±å‘Š:")
    print(classification_report(y_val_split, y_val_pred, 
                              target_names=['æ­»äº¡', 'å­˜æ´»']))

print(f"\nâœ¨ åˆ†æå®Œæˆï¼æ„Ÿè¬ä½¿ç”¨å®Œæ•´ç‰ˆéµé”å°¼è™Ÿç”Ÿå­˜é æ¸¬åˆ†æè…³æœ¬ï¼") 